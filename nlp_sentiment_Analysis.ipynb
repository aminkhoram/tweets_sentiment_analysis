{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwaJSWoMfBaN1XTqwxVaKF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminkhoram/tweets_sentiment_analysis/blob/main/nlp_sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s2eCRia6THK"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from os import getcwd\n",
        "import w1_unittest\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ],
      "metadata": {
        "id": "ZTmvGvQb9d5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "from utils import process_tweet, build_freqs"
      ],
      "metadata": {
        "id": "4qc7I3Mb9gtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "7t-fNk3U9lqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ],
      "metadata": {
        "id": "Jb5drxVR9x46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "zNPXj5Lf910T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ],
      "metadata": {
        "id": "Ir5ybUJj94Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "metadata": {
        "id": "KBN4jUrR-0Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function below\n",
        "print('This is an example of a positive tweet: \\n', train_x[0])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
      ],
      "metadata": {
        "id": "WLoJ3vzV-3C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    return h"
      ],
      "metadata": {
        "id": "rSSu_BOq-8yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1_unittest.test_sigmoid(sigmoid)"
      ],
      "metadata": {
        "id": "ONbT7e7w_dWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n",
        "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"
      ],
      "metadata": {
        "id": "4NeCXdtl_sEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: the final weight vector\n",
        "    '''\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = x.shape[0]\n",
        "\n",
        "    for i in range(0, num_iters):\n",
        "\n",
        "        # get z, the dot product of x and theta\n",
        "        z = no.dot(x,teta)\n",
        "\n",
        "        # get the sigmoid of z\n",
        "        h = sigmoid(z)\n",
        "\n",
        "        # calculate the cost function\n",
        "        J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha/m) * np.dot(x.T, (h-y))\n",
        "\n",
        "\n",
        "    J = float(J)\n",
        "    return J, theta"
      ],
      "metadata": {
        "id": "BOET2DOMDIcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Â Test the function\n",
        "w1_unittest.test_gradientDescent(gradientDescent)"
      ],
      "metadata": {
        "id": "yMkiwBdJE0-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C3 GRADED FUNCTION: extract_features\n",
        "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output:\n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # 3 elements for [bias, positive, negative] counts\n",
        "    x = np.zeros(3)\n",
        "\n",
        "    # bias term is set to 1\n",
        "    x[0] = 1\n",
        "\n",
        "\n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "\n",
        "        # increment the word count for the positive label 1\n",
        "        if (word, 1) in freqs:\n",
        "            x[1] += freqs[(word, 1)]\n",
        "\n",
        "        # increment the word count for the negative label 0\n",
        "        if (word, 0) in freqs:\n",
        "            x[2] += freqs[(word, 0)]\n",
        "\n",
        "\n",
        "    x = x[None, :]  # adding batch dimension for further processing\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x"
      ],
      "metadata": {
        "id": "JeP5e5_gIiC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "metadata": {
        "id": "vWFE2s9SI4hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C4 GRADED FUNCTION: predict_tweet\n",
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output:\n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "\n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet, freqs, process_tweet)\n",
        "\n",
        "    # make the prediction using x and theta\n",
        "    z = np.dot(x, theta)  # compute the dot product\n",
        "    y_pred = sigmoid(z)   # apply the sigmoid functio\n",
        "\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "tMfJeZn_Jclx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
      ],
      "metadata": {
        "id": "8TjYUzJHJiHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test2\n",
        "my_tweet = 'I am learning :)'\n",
        "predict_tweet(my_tweet, freqs, theta)"
      ],
      "metadata": {
        "id": "2yapDjSOJzKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "w1_unittest.test_predict_tweet(predict_tweet, freqs, theta)"
      ],
      "metadata": {
        "id": "ZFn0zcl6Lxkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C5 GRADED FUNCTION: test_logistic_regression\n",
        "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "\n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "\n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1.0)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0.0)\n",
        "\n",
        "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "    y_hat = np.array(y_hat)\n",
        "    test_y = np.array(test_y).flatten()  # Ensure test_y is a one-dimensional array\n",
        "    accuracy = np.mean(y_hat == test_y)\n",
        "\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "OUI0JeptL6Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "MZl-IXymL_Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test thefunction\n",
        "w1_unittest.unittest_test_logistic_regression(test_logistic_regression, freqs, theta)"
      ],
      "metadata": {
        "id": "8UjyIAdlMBhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some error analysis\n",
        "print('Label Predicted Tweet')\n",
        "for x,y in zip(test_x,test_y):\n",
        "    y_hat = predict_tweet(x, freqs, theta)\n",
        "\n",
        "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
        "        print('THE TWEET IS:', x)\n",
        "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
        "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
      ],
      "metadata": {
        "id": "1cPzx4dkMPuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
        "print(process_tweet(my_tweet))\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else:\n",
        "    print('Negative sentiment')"
      ],
      "metadata": {
        "id": "ma3m9YEaMTWR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}